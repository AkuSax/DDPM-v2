2025-06-23 02:56:28,040 - INFO - [Rank 0] Starting distributed training with 2 GPUs
2025-06-23 02:56:28,041 - INFO - [Rank 0] Loading dataset...
2025-06-23 02:56:28,053 - INFO - [Rank 0] Creating model...
2025-06-23 02:56:28,482 - INFO - [Rank 0] Setting up diffusion...
2025-06-23 02:56:28,507 - INFO - [Rank 0] Starting training...
2025-06-23 02:56:30,806 - ERROR - [Rank 0] Error during training: [Errno 2] No such file or directory: './data/manifest.csv'
2025-06-23 02:56:31,449 - ERROR - [Rank 1] Error during training: Failed running call_function <built-in method conv2d of type object at 0x762c936c61c0>(*(FakeTensor(..., device='cuda:1', size=(24, 1280, 4, 4), grad_fn=<CatBackward0>), Parameter(FakeTensor(..., device='cuda:1', size=(512, 1536, 3, 3), requires_grad=True)), None, (1, 1), (1, 1), (1, 1), 1), **{}):
Given groups=1, weight of size [512, 1536, 3, 3], expected input[24, 1280, 4, 4] to have 1536 channels, but got 1280 channels instead

from user code:
   File "/home/dmic/Fibrosis/Akul/FibLDM/unet2d.py", line 277, in forward
    x = self.upconv1(x)
  File "/home/dmic/Fibrosis/Akul/FibLDM/unet2d.py", line 30, in forward
    return self.double_conv(x)
  File "/home/dmic/miniconda3/envs/contourdiff/lib/python3.10/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/home/dmic/miniconda3/envs/contourdiff/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 554, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/dmic/miniconda3/envs/contourdiff/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 549, in _conv_forward
    return F.conv2d(

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-06-23 02:58:39,076 - INFO - [Rank 0] Starting distributed training with 2 GPUs
2025-06-23 02:58:39,077 - INFO - [Rank 0] Loading dataset...
2025-06-23 02:58:39,089 - INFO - [Rank 0] Creating model...
2025-06-23 02:58:39,534 - INFO - [Rank 0] Setting up diffusion...
2025-06-23 02:58:39,556 - INFO - [Rank 0] Starting training...
2025-06-23 02:58:42,378 - ERROR - [Rank 0] Error during training: Failed running call_function <built-in method conv2d of type object at 0x75ef834c61c0>(*(FakeTensor(..., device='cuda:0', size=(24, 1280, 4, 4), grad_fn=<CatBackward0>), Parameter(FakeTensor(..., device='cuda:0', size=(512, 1536, 3, 3), requires_grad=True)), None, (1, 1), (1, 1), (1, 1), 1), **{}):
Given groups=1, weight of size [512, 1536, 3, 3], expected input[24, 1280, 4, 4] to have 1536 channels, but got 1280 channels instead

from user code:
   File "/home/dmic/Fibrosis/Akul/FibLDM/unet2d.py", line 277, in forward
    x = self.upconv1(x)
  File "/home/dmic/Fibrosis/Akul/FibLDM/unet2d.py", line 30, in forward
    return self.double_conv(x)
  File "/home/dmic/miniconda3/envs/contourdiff/lib/python3.10/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/home/dmic/miniconda3/envs/contourdiff/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 554, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/dmic/miniconda3/envs/contourdiff/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 549, in _conv_forward
    return F.conv2d(

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-06-23 02:58:42,532 - ERROR - [Rank 1] Error during training: Failed running call_function <built-in method conv2d of type object at 0x7cdb7f8c61c0>(*(FakeTensor(..., device='cuda:1', size=(24, 1280, 4, 4), grad_fn=<CatBackward0>), Parameter(FakeTensor(..., device='cuda:1', size=(512, 1536, 3, 3), requires_grad=True)), None, (1, 1), (1, 1), (1, 1), 1), **{}):
Given groups=1, weight of size [512, 1536, 3, 3], expected input[24, 1280, 4, 4] to have 1536 channels, but got 1280 channels instead

from user code:
   File "/home/dmic/Fibrosis/Akul/FibLDM/unet2d.py", line 277, in forward
    x = self.upconv1(x)
  File "/home/dmic/Fibrosis/Akul/FibLDM/unet2d.py", line 30, in forward
    return self.double_conv(x)
  File "/home/dmic/miniconda3/envs/contourdiff/lib/python3.10/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/home/dmic/miniconda3/envs/contourdiff/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 554, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/dmic/miniconda3/envs/contourdiff/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 549, in _conv_forward
    return F.conv2d(

Set TORCH_LOGS="+dynamo" and TORCHDYNAMO_VERBOSE=1 for more information


You can suppress this exception and fall back to eager by setting:
    import torch._dynamo
    torch._dynamo.config.suppress_errors = True

2025-06-23 03:00:53,902 - INFO - [Rank 0] Starting distributed training with 2 GPUs
2025-06-23 03:00:53,904 - INFO - [Rank 0] Loading dataset...
2025-06-23 03:00:53,918 - INFO - [Rank 0] Creating model...
2025-06-23 03:00:54,342 - INFO - [Rank 0] Setting up diffusion...
2025-06-23 03:00:54,354 - INFO - [Rank 0] Starting training...
2025-06-23 03:01:12,741 - ERROR - [Rank 1] Error during training: The size of tensor a (8) must match the size of tensor b (3) at non-singleton dimension 1
2025-06-23 03:01:12,741 - ERROR - [Rank 0] Error during training: The size of tensor a (8) must match the size of tensor b (3) at non-singleton dimension 1
2025-06-23 03:39:28,513 - INFO - [Rank 0] Starting distributed training with 2 GPUs
2025-06-23 03:39:28,514 - INFO - [Rank 0] Loading dataset...
2025-06-23 03:39:28,525 - INFO - [Rank 0] Creating model...
2025-06-23 03:39:28,964 - INFO - [Rank 0] Setting up diffusion...
2025-06-23 03:39:28,979 - INFO - [Rank 0] Starting training...
2025-06-23 03:39:35,299 - ERROR - [Rank 0] Error during training: The size of tensor a (8) must match the size of tensor b (3) at non-singleton dimension 1
2025-06-23 03:39:35,421 - ERROR - [Rank 1] Error during training: The size of tensor a (8) must match the size of tensor b (3) at non-singleton dimension 1
2025-06-23 03:41:27,345 - INFO - [Rank 0] Starting distributed training with 2 GPUs
2025-06-23 03:41:27,345 - INFO - [Rank 0] Loading dataset...
2025-06-23 03:41:27,357 - INFO - [Rank 0] Creating model...
2025-06-23 03:41:27,781 - INFO - [Rank 0] Setting up diffusion...
2025-06-23 03:41:27,801 - INFO - [Rank 0] Starting training...
2025-06-23 03:41:54,209 - ERROR - [Rank 1] Error during training: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameter indices which did not receive grad for rank 1: 0 1
 In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
2025-06-23 03:41:54,416 - ERROR - [Rank 0] Error during training: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
making sure all `forward` function outputs participate in calculating loss. 
If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
Parameter indices which did not receive grad for rank 0: 0 1
 In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
2025-06-23 03:43:32,395 - INFO - [Rank 0] Starting distributed training with 2 GPUs
2025-06-23 03:43:32,396 - INFO - [Rank 0] Loading dataset...
2025-06-23 03:43:32,407 - INFO - [Rank 0] Creating model...
2025-06-23 03:43:32,847 - INFO - [Rank 0] Setting up diffusion...
2025-06-23 03:43:32,865 - INFO - [Rank 0] Starting training...
2025-06-23 03:43:46,202 - INFO - Epoch 0 | Train loss: 0.473913
2025-06-23 03:43:57,003 - INFO - Epoch 0 | Validation loss: 0.242635
2025-06-23 03:43:57,017 - INFO - Generating debug samples...
2025-06-23 03:43:57,018 - ERROR - [Rank 0] Error during training: 'EMA' object has no attribute 'ema_model'
2025-06-23 03:46:06,330 - INFO - [Rank 0] Starting distributed training with 2 GPUs
2025-06-23 03:46:06,331 - INFO - [Rank 0] Loading dataset...
2025-06-23 03:46:06,342 - INFO - [Rank 0] Creating model...
2025-06-23 03:46:06,771 - INFO - [Rank 0] Setting up diffusion...
2025-06-23 03:46:06,790 - INFO - [Rank 0] Starting training...
2025-06-23 03:46:20,211 - INFO - Epoch 0 | Train loss: 0.473870
2025-06-23 03:46:21,653 - INFO - Epoch 0 | Validation loss: 0.242394
2025-06-23 03:46:21,670 - INFO - Generating debug samples...
2025-06-23 03:46:34,389 - INFO - Saved decoded debug samples for epoch 0.
2025-06-23 03:46:34,390 - INFO - Epoch 0: Calculating metrics...
2025-06-23 03:46:34,391 - ERROR - [Rank 0] Error during training: [Errno 2] No such file or directory: './data/PT002-2017.09.25_5cm_00_slice_79.mat'
2025-06-23 03:48:16,337 - INFO - [Rank 0] Starting distributed training with 2 GPUs
2025-06-23 03:48:16,338 - INFO - [Rank 0] Loading dataset...
2025-06-23 03:48:16,348 - INFO - [Rank 0] Creating model...
2025-06-23 03:48:16,782 - INFO - [Rank 0] Setting up diffusion...
2025-06-23 03:48:16,802 - INFO - [Rank 0] Starting training...
2025-06-23 03:48:30,238 - INFO - Epoch 0 | Train loss: 0.473890
2025-06-23 03:48:31,727 - INFO - Epoch 0 | Validation loss: 0.242477
2025-06-23 03:48:31,740 - INFO - Generating debug samples...
2025-06-23 03:48:44,498 - INFO - Saved decoded debug samples for epoch 0.
2025-06-23 03:48:44,498 - INFO - Epoch 0: Calculating metrics...
2025-06-23 03:48:44,499 - ERROR - [Rank 0] Error during training: [Errno 2] No such file or directory: './data/PT002-2017.09.25_5cm_00_slice_79.mat'
2025-06-23 03:53:38,563 - INFO - [Rank 0] Starting distributed training with 2 GPUs
2025-06-23 03:53:38,563 - INFO - [Rank 0] Loading dataset...
2025-06-23 03:53:38,563 - ERROR - [Rank 0] Error during training: Manifest file not found at ../data/latents_dataset/manifest.csv. Please run `scripts/encode_dataset.py` first.
2025-06-23 03:53:38,689 - ERROR - [Rank 1] Error during training: Manifest file not found at ../data/latents_dataset/manifest.csv. Please run `scripts/encode_dataset.py` first.
2025-06-23 03:56:05,436 - INFO - [Rank 0] Starting distributed training with 2 GPUs
2025-06-23 03:56:05,437 - INFO - [Rank 0] Loading dataset...
2025-06-23 03:56:05,437 - ERROR - [Rank 0] Error during training: Manifest file not found at ../data/latents_dataset/manifest.csv. Please run `scripts/encode_dataset.py` first.
2025-06-23 03:56:05,542 - ERROR - [Rank 1] Error during training: Manifest file not found at ../data/latents_dataset/manifest.csv. Please run `scripts/encode_dataset.py` first.
2025-06-23 03:56:53,924 - INFO - [Rank 0] Starting distributed training with 2 GPUs
2025-06-23 03:56:53,924 - INFO - [Rank 0] Loading dataset...
2025-06-23 03:56:53,936 - INFO - [Rank 0] Creating model...
2025-06-23 03:56:54,362 - INFO - [Rank 0] Setting up diffusion...
2025-06-23 03:56:54,378 - INFO - [Rank 0] Starting training...
2025-06-23 03:56:56,256 - ERROR - [Rank 0] Error during training: [Errno 2] No such file or directory: './data/manifest.csv'
2025-06-23 03:57:55,669 - INFO - [Rank 0] Starting distributed training with 2 GPUs
2025-06-23 03:57:55,670 - INFO - [Rank 0] Loading dataset...
2025-06-23 03:57:55,684 - INFO - [Rank 0] Creating model...
2025-06-23 03:57:56,108 - INFO - [Rank 0] Setting up diffusion...
2025-06-23 03:57:56,130 - INFO - [Rank 0] Starting training...
2025-06-23 03:58:09,705 - INFO - Epoch 0 | Train loss: 0.473874
2025-06-23 03:58:11,219 - INFO - Epoch 0 | Validation loss: 0.242530
2025-06-23 03:58:11,232 - INFO - Generating debug samples...
2025-06-23 03:58:23,989 - INFO - Saved decoded debug samples for epoch 0.
2025-06-23 03:58:24,000 - ERROR - [Rank 0] Error during training: [Errno 2] No such file or directory: './data/latents/0.pt'
2025-06-23 03:59:07,004 - INFO - [Rank 0] Starting distributed training with 2 GPUs
2025-06-23 03:59:07,004 - INFO - [Rank 0] Loading dataset...
2025-06-23 03:59:07,016 - INFO - [Rank 0] Creating model...
2025-06-23 03:59:07,444 - INFO - [Rank 0] Setting up diffusion...
2025-06-23 03:59:07,470 - INFO - [Rank 0] Starting training...
2025-06-23 03:59:20,915 - INFO - Epoch 0 | Train loss: 0.473876
2025-06-23 03:59:22,386 - INFO - Epoch 0 | Validation loss: 0.242494
2025-06-23 03:59:22,398 - INFO - Generating debug samples...
2025-06-23 03:59:35,182 - INFO - Saved decoded debug samples for epoch 0.
2025-06-23 03:59:35,192 - ERROR - [Rank 0] Error during training: [Errno 2] No such file or directory: './data/latents/0.pt'
2025-06-23 04:00:08,854 - INFO - [Rank 0] Starting distributed training with 2 GPUs
2025-06-23 04:00:08,855 - INFO - [Rank 0] Loading dataset...
2025-06-23 04:00:08,867 - INFO - [Rank 0] Creating model...
2025-06-23 04:00:09,300 - INFO - [Rank 0] Setting up diffusion...
2025-06-23 04:00:09,321 - INFO - [Rank 0] Starting training...
2025-06-23 04:00:22,787 - INFO - Epoch 0 | Train loss: 0.473886
2025-06-23 04:00:24,236 - INFO - Epoch 0 | Validation loss: 0.242489
2025-06-23 04:00:24,248 - INFO - Generating debug samples...
2025-06-23 04:00:37,057 - INFO - Saved decoded debug samples for epoch 0.
2025-06-23 04:00:37,084 - ERROR - [Rank 0] Error during training: Expected 3D (unbatched) or 4D (batched) input to conv_transpose2d, but got input of size: [1, 1, 8, 16, 16]
2025-06-23 04:02:27,750 - INFO - [Rank 0] Starting distributed training with 2 GPUs
2025-06-23 04:02:27,751 - INFO - [Rank 0] Loading dataset...
2025-06-23 04:02:27,762 - INFO - [Rank 0] Creating model...
2025-06-23 04:02:28,185 - INFO - [Rank 0] Setting up diffusion...
2025-06-23 04:02:28,204 - INFO - [Rank 0] Starting training...
2025-06-23 04:02:41,764 - INFO - Epoch 0 | Train loss: 0.473859
2025-06-23 04:02:43,227 - INFO - Epoch 0 | Validation loss: 0.242813
2025-06-23 04:02:43,242 - INFO - Generating debug samples...
2025-06-23 04:02:56,088 - INFO - Saved decoded debug samples for epoch 0.
2025-06-23 04:03:04,876 - INFO - Computing FID, KID, LPIPS, SSIM...
2025-06-23 04:03:04,876 - ERROR - [Rank 0] Error during training: 'RealismMetrics' object has no attribute 'compute'
2025-06-23 04:03:47,211 - INFO - [Rank 0] Starting distributed training with 2 GPUs
2025-06-23 04:03:47,212 - INFO - [Rank 0] Loading dataset...
2025-06-23 04:03:47,223 - INFO - [Rank 0] Creating model...
2025-06-23 04:03:47,653 - INFO - [Rank 0] Setting up diffusion...
2025-06-23 04:03:47,665 - INFO - [Rank 0] Starting training...
2025-06-23 04:04:01,290 - INFO - Epoch 0 | Train loss: 0.473869
2025-06-23 04:04:02,758 - INFO - Epoch 0 | Validation loss: 0.242597
2025-06-23 04:04:02,771 - INFO - Generating debug samples...
2025-06-23 04:04:15,643 - INFO - Saved decoded debug samples for epoch 0.
2025-06-23 04:04:24,387 - INFO - Computing FID, KID, LPIPS, SSIM...
2025-06-23 04:04:25,063 - WARNING - [metrics.py] FID computation error: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor
2025-06-23 04:04:43,619 - WARNING - [metrics.py] LPIPS computation error: Encountered different devices in metric calculation (see stacktrace for details). This could be due to the metric class not being on the same device as input. Instead of `metric=LearnedPerceptualImagePatchSimilarity(...)` try to do `metric=LearnedPerceptualImagePatchSimilarity(...).to(device)` where device corresponds to the device of the input.
2025-06-23 04:04:44,743 - INFO - Epoch 0 Metrics | fid: inf | kid: 0.7990 | kid_std: 0.0146 | lpips: inf | ssim: 0.0184
2025-06-23 04:04:51,147 - INFO - Epoch 1 | Train loss: 0.283905
2025-06-23 04:04:51,272 - INFO - Epoch 1 | Validation loss: 0.214572
2025-06-23 04:04:57,847 - INFO - Epoch 2 | Train loss: 0.246483
2025-06-23 04:04:57,972 - INFO - Epoch 2 | Validation loss: 0.207423
2025-06-23 04:05:04,530 - INFO - Epoch 3 | Train loss: 0.232672
2025-06-23 04:05:04,668 - INFO - Epoch 3 | Validation loss: 0.190380
2025-06-23 04:05:11,248 - INFO - Epoch 4 | Train loss: 0.219195
2025-06-23 04:05:11,373 - INFO - Epoch 4 | Validation loss: 0.166911
2025-06-23 04:05:17,890 - INFO - Epoch 5 | Train loss: 0.208928
2025-06-23 04:05:18,016 - INFO - Epoch 5 | Validation loss: 0.167528
2025-06-23 04:05:24,548 - INFO - Epoch 6 | Train loss: 0.202788
2025-06-23 04:05:24,677 - INFO - Epoch 6 | Validation loss: 0.162411
2025-06-23 04:05:31,069 - INFO - Epoch 7 | Train loss: 0.186698
2025-06-23 04:05:31,199 - INFO - Epoch 7 | Validation loss: 0.149087
2025-06-23 04:05:37,778 - INFO - Epoch 8 | Train loss: 0.186648
2025-06-23 04:05:37,907 - INFO - Epoch 8 | Validation loss: 0.152240
2025-06-23 04:05:44,192 - INFO - Epoch 9 | Train loss: 0.183519
2025-06-23 04:05:44,316 - INFO - Epoch 9 | Validation loss: 0.145897
2025-06-23 04:05:50,483 - INFO - Epoch 10 | Train loss: 0.179491
2025-06-23 04:05:50,618 - INFO - Epoch 10 | Validation loss: 0.135422
2025-06-23 04:05:56,676 - INFO - Epoch 11 | Train loss: 0.172352
2025-06-23 04:05:56,797 - INFO - Epoch 11 | Validation loss: 0.135401
2025-06-23 04:06:02,928 - INFO - Epoch 12 | Train loss: 0.168063
2025-06-23 04:06:03,052 - INFO - Epoch 12 | Validation loss: 0.134138
2025-06-23 04:06:09,362 - INFO - Epoch 13 | Train loss: 0.166351
2025-06-23 04:06:09,487 - INFO - Epoch 13 | Validation loss: 0.122024
2025-06-23 04:06:15,689 - INFO - Epoch 14 | Train loss: 0.161588
2025-06-23 04:06:15,813 - INFO - Epoch 14 | Validation loss: 0.119370
2025-06-23 04:06:22,287 - INFO - Epoch 15 | Train loss: 0.159534
2025-06-23 04:06:22,419 - INFO - Epoch 15 | Validation loss: 0.120027
2025-06-23 04:06:28,938 - INFO - Epoch 16 | Train loss: 0.156138
2025-06-23 04:06:29,062 - INFO - Epoch 16 | Validation loss: 0.128037
2025-06-23 04:06:35,540 - INFO - Epoch 17 | Train loss: 0.157707
2025-06-23 04:06:35,676 - INFO - Epoch 17 | Validation loss: 0.116314
2025-06-23 04:06:42,312 - INFO - Epoch 18 | Train loss: 0.159140
2025-06-23 04:06:42,438 - INFO - Epoch 18 | Validation loss: 0.115831
2025-06-23 04:06:48,964 - INFO - Epoch 19 | Train loss: 0.154610
2025-06-23 04:06:49,090 - INFO - Epoch 19 | Validation loss: 0.117698
2025-06-23 04:06:55,692 - INFO - Epoch 20 | Train loss: 0.152089
2025-06-23 04:06:55,820 - INFO - Epoch 20 | Validation loss: 0.114745
2025-06-23 04:07:02,531 - INFO - Epoch 21 | Train loss: 0.143117
2025-06-23 04:07:02,660 - INFO - Epoch 21 | Validation loss: 0.115607
2025-06-23 04:07:08,954 - INFO - Epoch 22 | Train loss: 0.146901
2025-06-23 04:07:09,082 - INFO - Epoch 22 | Validation loss: 0.112961
2025-06-23 04:07:15,652 - INFO - Epoch 23 | Train loss: 0.143752
2025-06-23 04:07:15,787 - INFO - Epoch 23 | Validation loss: 0.110089
2025-06-23 04:07:22,440 - INFO - Epoch 24 | Train loss: 0.146140
2025-06-23 04:07:22,561 - INFO - Epoch 24 | Validation loss: 0.106686
2025-06-23 04:07:29,008 - INFO - Epoch 25 | Train loss: 0.145067
2025-06-23 04:07:29,134 - INFO - Epoch 25 | Validation loss: 0.099498
2025-06-23 04:07:29,147 - INFO - Generating debug samples...
2025-06-23 04:07:41,934 - INFO - Saved decoded debug samples for epoch 25.
2025-06-23 04:07:50,701 - INFO - Computing FID, KID, LPIPS, SSIM...
2025-06-23 04:07:51,373 - WARNING - [metrics.py] FID computation error: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor
2025-06-23 04:08:09,869 - WARNING - [metrics.py] LPIPS computation error: Encountered different devices in metric calculation (see stacktrace for details). This could be due to the metric class not being on the same device as input. Instead of `metric=LearnedPerceptualImagePatchSimilarity(...)` try to do `metric=LearnedPerceptualImagePatchSimilarity(...).to(device)` where device corresponds to the device of the input.
2025-06-23 04:08:11,008 - INFO - Epoch 25 Metrics | fid: inf | kid: 1.0408 | kid_std: 0.0239 | lpips: inf | ssim: 0.0188
2025-06-23 04:08:17,380 - INFO - Epoch 26 | Train loss: 0.143384
2025-06-23 04:08:17,507 - INFO - Epoch 26 | Validation loss: 0.111415
2025-06-23 04:08:24,000 - INFO - Epoch 27 | Train loss: 0.142792
2025-06-23 04:08:24,127 - INFO - Epoch 27 | Validation loss: 0.097949
2025-06-23 04:08:30,602 - INFO - Epoch 28 | Train loss: 0.140971
2025-06-23 04:08:30,729 - INFO - Epoch 28 | Validation loss: 0.098892
2025-06-23 04:08:37,041 - INFO - Epoch 29 | Train loss: 0.140985
2025-06-23 04:08:37,170 - INFO - Epoch 29 | Validation loss: 0.105548
2025-06-23 04:08:43,796 - INFO - Epoch 30 | Train loss: 0.139368
2025-06-23 04:08:43,924 - INFO - Epoch 30 | Validation loss: 0.103408
2025-06-23 04:08:50,306 - INFO - Epoch 31 | Train loss: 0.139031
2025-06-23 04:08:50,432 - INFO - Epoch 31 | Validation loss: 0.108143
2025-06-23 04:08:57,062 - INFO - Epoch 32 | Train loss: 0.137481
2025-06-23 04:08:57,188 - INFO - Epoch 32 | Validation loss: 0.102838
2025-06-23 04:09:03,773 - INFO - Epoch 33 | Train loss: 0.136399
2025-06-23 04:09:03,903 - INFO - Epoch 33 | Validation loss: 0.102004
2025-06-23 04:09:10,092 - INFO - Epoch 34 | Train loss: 0.133598
2025-06-23 04:09:10,215 - INFO - Epoch 34 | Validation loss: 0.103608
2025-06-23 04:09:16,912 - INFO - Epoch 35 | Train loss: 0.136482
2025-06-23 04:09:17,042 - INFO - Epoch 35 | Validation loss: 0.104955
2025-06-23 04:09:23,579 - INFO - Epoch 36 | Train loss: 0.136141
2025-06-23 04:09:23,710 - INFO - Epoch 36 | Validation loss: 0.106017
2025-06-23 04:09:30,092 - INFO - Epoch 37 | Train loss: 0.136334
2025-06-23 04:09:30,216 - INFO - Epoch 37 | Validation loss: 0.103474
2025-06-23 04:09:30,216 - INFO - Early stopping triggered by validation loss.
2025-06-23 04:19:30,332 - ERROR - [Rank 0] Error during training: tensor or list of tensors expected, got <class 'torch._dynamo.eval_frame.OptimizedModule'>
