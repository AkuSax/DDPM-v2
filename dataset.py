import os
import pandas as pd
from torchvision.io import read_image
import torch
from torch.utils.data import Dataset
from torchvision import datasets
from torchvision.transforms import ToTensor
import matplotlib.pyplot as plt
from utils import readmat
from torch.utils.data.sampler import Sampler
import numpy as np
from torchvision import transforms
import torchvision.transforms.v2 as T
import torch.nn.functional as F

class NonUniformScaling: 
    """
    Apply non-uniform scaling on either direction (vertical or horizontal).

    Parameters:
    scale_x_range (tuple): Range of horizontal scaling.
    scale_y_range (tuple): Range of vertical scaling.

    Returns:
    torch.Tensor: image after scaling.
    """
    def __init__(self, scale_x_range, scale_y_range):
        self.scale_x_range = scale_x_range
        self.scale_y_range = scale_y_range

    def __call__(self, img_tensor):
        # Generate random scaling factors using PyTorch
        scale_x = (self.scale_x_range[1] - self.scale_x_range[0]) * torch.rand(1).item() + self.scale_x_range[0]
        scale_y = (self.scale_y_range[1] - self.scale_y_range[0]) * torch.rand(1).item() + self.scale_y_range[0]

        # Create the 2D affine transformation matrix for scaling
        theta = torch.tensor([
            [scale_x, 0, 0],
            [0, scale_y, 0]
        ], dtype=torch.float).unsqueeze(0)  # Add batch dimension

        # Create the affine grid
        grid = torch.nn.functional.affine_grid(theta, img_tensor.size(), align_corners=True)

        # Apply the affine transformation
        stretched_img_tensor = torch.nn.functional.grid_sample(img_tensor, grid, align_corners=True)

        return stretched_img_tensor

class ContourDataset(Dataset):
    def __init__(self, label_file, img_dir, istransform=True):
        self.img_labels = pd.read_csv(label_file) # list file
        self.img_dir   = img_dir # data folder
        self.istransform = istransform
        
        # A more standard and stable augmentation pipeline for medical images
        self.transforms = transforms.Compose([
            # Geometric augmentations
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomAffine(
                degrees=5,          # Reduced rotation
                translate=(0.05, 0.05) # Reduced translation
            ),
            # Intensity augmentation (clamping to prevent outliers)
            transforms.Lambda(lambda x: torch.clamp(x, -1.0, 1.0)),
        ])


    def __len__(self): # total data number
        return len(self.img_labels)

    def __getitem__(self, idx):
        # print(self.img_dir)
        # assert False
        img_name = self.img_labels.iloc[idx, 0]
        c_name = self.img_labels.iloc[idx, 1]
        # print(img_name)
        # print(c_name)
        img_path = os.path.join(self.img_dir, img_name)
        c_path = os.path.join(self.img_dir, c_name)
        # print(img_path)
        # print(c_path)
        # assert False
        volume = readmat(img_path)
        contour = readmat(c_path)

        if self.istransform:
            rng_state = torch.random.get_rng_state()

            volume = volume.unsqueeze(0)
            contour = contour.unsqueeze(0)
            volume = self.transforms(volume)
            torch.random.set_rng_state(rng_state)
            contour = self.transforms(contour)

            volume = volume.squeeze(0)
            contour = contour.squeeze(0)
            
            volume = (volume - volume.min()) / (volume.max() - volume.min())
            # Normalize to [-1, 1] for diffusion model
            volume = volume * 2.0 - 1.0
            volume = volume.float()

            contour = (contour>0.2).float()

        return volume, contour

class LatentDataset(Dataset):
    """
    A dataset that loads pre-computed latent representations and their
    corresponding contour maps. The data is expected to be generated by
    the `scripts/encode_dataset.py` script.
    """
    def __init__(self, data_dir, downsample_contour=False, latent_size=16):
        self.latent_dir = os.path.join(data_dir, "latents")
        self.contour_dir = os.path.join(data_dir, "contours")
        self.manifest_path = os.path.join(data_dir, "manifest.csv")
        
        if not os.path.exists(self.manifest_path):
            raise FileNotFoundError(f"Manifest file not found at {self.manifest_path}. Please run `scripts/encode_dataset.py` first.")
            
        self.manifest = pd.read_csv(self.manifest_path)
        self.num_files = len(self.manifest)
        self.downsample_contour = downsample_contour
        self.latent_size = latent_size

    def __len__(self):
        return self.num_files

    def __getitem__(self, idx):
        latent_path = os.path.join(self.latent_dir, f"{idx}.pt")
        contour_path = os.path.join(self.contour_dir, f"{idx}.pt")
        
        latent = torch.load(latent_path)
        contour = torch.load(contour_path)

        # The VAE output is (1, latent_dim, 16, 16), so we squeeze the batch dim
        latent = latent.squeeze(0)

        if self.downsample_contour:
            # Downsample contour to match the spatial dimensions of the latent vector
            # This is now only used for backward compatibility
            contour = F.interpolate(contour.unsqueeze(0), size=(self.latent_size, self.latent_size), mode='nearest')
            contour = contour.squeeze(0)
            
        return latent, contour
    